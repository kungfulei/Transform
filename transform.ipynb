{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "sys.version_info(major=3, minor=7, micro=2, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.2\n",
      "pandas 1.0.3\n",
      "sklearn 0.22.2.post1\n",
      "tensorflow 2.1.0\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.loads data\n",
    "#.preprocess data->dataset\n",
    "#3.tools\n",
    "#3.1 generates position embedding\n",
    "#3.2 create mask.(a.padding,b.decoder)\n",
    "#3.3 scaled_dot_product_attention\n",
    "#4.builds model\n",
    "#4.1 MultiheadAttention\n",
    "#4.2 EncoderLayer\n",
    "#4.3 DecoderLayer\n",
    "#4.4 EncoderModel\n",
    "#4.5 DecoderModel\n",
    "#4.6 Transformer\n",
    "#5.optimizer &loss\n",
    "#6.train step ->train\n",
    "#7.Evaluate and Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spa_file_path='./spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in  unicodedata.normalize('NFD',s) if unicodedata.category(c) !='Mn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s=unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    #标点符号前后加空格\n",
    "    s=re.sub(r\"([?.!,])\",r\" \\1 \",s)\n",
    "    \n",
    "    #多余的空格变成一个空格\n",
    "    s=re.sub(r'[\" \"]+',\" \",s)\n",
    "    #去掉前后空格\n",
    "    s=s.rstrip().strip()\n",
    "    s='<start> '+s+ ' <end>'\n",
    "    return s\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines=open(filename,encoding='utf-8').read().strip().split('\\n')\n",
    "    sentence_pairs=[line.split('\\t') for line in lines]\n",
    "   \n",
    "    preprocessed_sentence_pairs=[ \n",
    "        (preprocess_sentence(en),preprocess_sentence(sp) )for en,sp in sentence_pairs]\n",
    "    \n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset,sp_dataset=parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer=keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None,filters='',split=' ')# num_words指定多少单词\n",
    "    lang_tokenizer.fit_on_texts(lang)#根据词频生成词表\n",
    "    tensor=lang_tokenizer.texts_to_sequences(lang)#生成句子id表示\n",
    "    print(tensor[1])\n",
    "    tensor=keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    print(tensor[2])\n",
    "    return tensor,lang_tokenizer\n",
    "\n",
    "input_tensor,input_tokenizer=tokenizer(sp_dataset[0:30000])\n",
    "output_tensor,output_tokenizer=tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input=max_length(input_tensor)\n",
    "max_length_output=max_length(output_tensor)\n",
    "print(max_length_input,max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train ,input_eval,output_train,output_eval=train_test_split(input_tensor,output_tensor,test_size=0.2)\n",
    "print(len(input_train),len(input_eval),len(output_train),len(output_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(example,tokenizer):\n",
    "    for t in example:\n",
    "        if t !=0:\n",
    "            print(\"{}-->{}\".format(t,tokenizer.index_word[t]))\n",
    "            \n",
    "convert(input_train[5],input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor,output_tensor,batch_size,epochs,shuffle):\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((input_tensor,output_tensor))\n",
    "    if shuffle:\n",
    "        dataset=dataset.shuffle(30000)\n",
    "    dataset=dataset.repeat(epochs).batch(batch_size,drop_remainder=True)\n",
    "    return dataset\n",
    "    \n",
    "batch_size=64\n",
    "epochs=20\n",
    "    \n",
    "train_dataset=make_dataset(input_train,output_train,batch_size,epochs,True)\n",
    "eval_dataset=make_dataset(input_eval,output_eval,batch_size,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PE(pos,2i)=sin(pos/10000^(2i/d_modek))\n",
    "#PE(pos,2i+1)=cos(pos/10000^(2i/d_modek))\n",
    "\n",
    "#pos:[sententce_length,1]\n",
    "#i.shape :[1,d_model]\n",
    "#resule.shape:[sententce_length,d_model]\n",
    "def get_angles(pos,i,d_model):#pos词语在句子中的位置，i是在embedding中的位置,d_model是embedding的大小\n",
    "    angle_rates=1/np.power(10000,(2*(i//2))/np.float32(d_model))       \n",
    "    return pos*angle_rates\n",
    "def get_positional_embedding(sentence_length,d_model):\n",
    "    angle_rads=get_angles(np.arange(sentence_length)[:,np.newaxis],\n",
    "                         np.arange(d_model)[np.newaxis,:],d_model)#np.arange(sentence_length)[:,np.newaxis]绛sentence——length向量扩展成矩阵\n",
    "    #sines.shape:[sentence_length,d_model/2]\n",
    "    #cosine.shape:[sentence_length,d_model/2]\n",
    "    sines=np.sin(angle_rads[:,0::2])\n",
    "    cosines=np.cos(angle_rads[:,1::2])   \n",
    "    #position_embedding.shape:[sentence_length,d_model]\n",
    "    position_embedding=np.concatenate([sines,cosines],axis=-1)\n",
    "    #position_embedding.shape:[1,sentence_length,d_model]\n",
    "    position_embedding=position_embedding[np.newaxis,...]\n",
    "    \n",
    "    return tf.cast(position_embedding,dtype=tf.float32)#转化类型\n",
    "position_embedding=get_positional_embedding(50,512)\n",
    "\n",
    "print(position_embedding.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_position_embedding(position_embedding):\n",
    "    plt.pcolormesh(position_embedding[0],cmap='RdBu')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0,512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "plot_position_embedding(position_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.padding mask,2.look ahead(decoder只能和之前的词语发生关系，不能和之后的词语发生关系)\n",
    "\n",
    "#batch_data.shape:[batch_size,seq_len]\n",
    "\n",
    "def create_padding_mask(batch_data):\n",
    "    \n",
    "    padding_mask=tf.cast(tf.math.equal(batch_data,0),tf.float32)\n",
    "    #[batch_size,1,1,seq_len]\n",
    "    return padding_mask[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "x=tf.constant([[4,45,3,0,0],[5,0,34,3,0],[89,0,3,0,1]])\n",
    "create_padding_mask(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_weights.shape:[3,3]\n",
    "#[[1,0,0],1表示第一个单词与本身的关系，其余类推\n",
    "#[4,4,0],\n",
    "#[3,5,4]]\n",
    "def create_look_ahead_mask(size):\n",
    "    mask=1-tf.linalg.band_part(tf.ones((size,size)),-1,0)\n",
    "    return mask\n",
    "\n",
    "create_look_ahead_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q:shape==(...,seq_len_q,depth)  q和k要做乘法，最后一个维度要相等\n",
    "        k:shape==(...,seq_len_k,depth)\n",
    "        v:shape==(...,seq_len_v,depth_v) \n",
    "        seq_len_k ==seq_len_v\n",
    "        mask:shape==(...,seq_len_q,seq_len_k)\n",
    "    \"\"\"\n",
    "    matmul_qk=tf.matmul(q,k,transpose_b=True)\n",
    "    dk=tf.cast(tf.shape(k)[-1],tf.float32)#dk 是k最后一维的维度\n",
    "    scaled_attention_logits=matmul_qk/tf.math.sqrt(dk)   \n",
    "    if mask is not None:\n",
    "        #使得在softmax后值趋近于0\n",
    "        scaled_attention_logits+=(mask * -1e9)#为什么要用加法而不是乘法呢？logits以后做softmax时候，由于加上-le9，会让他的值无线接近于零\n",
    "    #attention_weights.shape:(...,seq_len_q,seq_len_k)\n",
    "    attention_weights=tf.nn.softmax(scaled_attention_logits,axis=-1)#在最后一个维度做softmax\n",
    "    #output.shape:(...,seq_len_q,depth_v)\n",
    "    output=tf.matmul(attention_weights,v)#矩阵乘法只作用于后两位，前面维度忽略掉\n",
    "    return output,attention_weights\n",
    "def print_scaled_dot_product_attention(q,k,v):\n",
    "    temp_out,temp_att=scaled_dot_product_attention(q,k,v,None)\n",
    "    print('temp_out',temp_out)\n",
    "    print('``````')\n",
    "    print('temp_att',temp_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k=tf.constant([[10,0,0],\n",
    "                   [0,10,0],\n",
    "                   [0,0,10],\n",
    "                   [0,0,10]],dtype=tf.float32)\n",
    "temp_v=tf.constant([[1,0],\n",
    "                   [10,0],\n",
    "                   [100,5],\n",
    "                   [1000,6]],dtype=tf.float32)\n",
    "temp_q1=tf.constant([[0,10,0]],dtype=tf.float32)\n",
    "np.set_printoptions(suppress=True)#改变显示结果\n",
    "print_scaled_dot_product_attention(temp_q1,temp_k,temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上：\n",
    "    x->Wq0->q0\n",
    "    x->Wk0->k0\n",
    "    x->Wv0->v0   \n",
    "    实战中：\n",
    "    q->Wq0->q0\n",
    "    k->Wk0->k0\n",
    "    v->Wv0->v0\n",
    "    实战中技巧;\n",
    "    q->wq->Q->split->q0,q1,q2  \n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.d_model=d_model\n",
    "        assert d_model % self.num_heads==0\n",
    "        self.depth=d_model//self.num_heads\n",
    "        self.WQ=keras.layers.Dense(self.d_model)\n",
    "        self.WK=keras.layers.Dense(self.d_model)\n",
    "        self.WV=keras.layers.Dense(self.d_model)\n",
    "        self.dense=keras.layers.Dense(self.d_model)\n",
    "    def split_heads(self,x,batch_size):\n",
    "        #x.shape:(batch_size,seq_len,d_model)\n",
    "        #d_mdoel=num_heads*depth\n",
    "        #x->(batch_size,num_heads,seq_len,depth)\n",
    "        x=tf.reshape(x,(batch_size,-1,self.num_heads,self.depth))#d_model->num_heads,depth\n",
    "        return tf.transpose(x,perm=[0,2,1,3])\n",
    "    def call(self,q,k,v,mask):\n",
    "        batch_size=tf.shape(q)[0]#???\n",
    "        #\n",
    "        q=self.WQ(q)#q.shape:(batch_size,seq_len_q,d_model)\n",
    "        k=self.WK(k)#k.shape:(batch_size,seq_len_k,d_model)\n",
    "        v=self.WV(v)#v.shape:(batch_size,seq_len_v,d_model)\n",
    "        #q.shape:(batch_size,num_heads,seq_len_q,depth)\n",
    "        q=self.split_heads(q,batch_size)\n",
    "        k=self.split_heads(k,batch_size)\n",
    "        v=self.split_heads(v,batch_size)\n",
    "        \n",
    "        #scaled_dot_product_attention.shape:(batch_size,num_heads,seq_len_q,depth)多头信息存在第二维和第4维上\n",
    "        #attention_wights.shape:(batch_size,num_heads,seq_len_q,seq_len_k)\n",
    "        scaled_attention_outputs,attention_weights=scaled_dot_product_attention(q,k,v,mask)\n",
    "        #scaled_attention_outputs.shape:(batch_size,seq_len_q,num_heads,depth)\n",
    "        scaled_attention_outputs=tf.transpose(scaled_attention_outputs,perm=[0,2,1,3])\n",
    "        #concat_attention.shape:(batch_size,seq_len_q,d_model)\n",
    "        concat_attention=tf.reshape(scaled_attention_outputs,(batch_size,-1,self.d_model))\n",
    "        #output.shape:(batch_size,seq_len_q,d_model)\n",
    "        output=self.dense(concat_attention)\n",
    "        return output,attention_weights\n",
    "        \n",
    "temp_mpa=MultiHeadAttention(d_model=512,num_heads=8)\n",
    "y=tf.random.uniform((1,60,256))#(batch_size,seq_len_q,dim)\n",
    "output,attn=temp_mpa(y,y,y,mask=None)\n",
    "print(output.shape)\n",
    "print(attn.shape)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(d_model,dff):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(dff,activation='relu'),\n",
    "        keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x->self attention ->add &normalize&dropout\\\n",
    "    ->feed_forward->add&normalize &dropout\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads,dff,rate=0.1):#rate,dropout\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha=MultiHeadAttention(d_model,num_heads)\n",
    "        self.ffn=feed_forward_network(d_model,dff)\n",
    "        \n",
    "        self.layer_norml=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norml=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.dropout1=keras.layers.Dropout(rate)\n",
    "        self.dropout2=keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self,x,training,encoder_padding_mask):\n",
    "        #x.shape:(batch_size,seq_len,dim=d_model输出结果一样才可以做加法)\n",
    "        #attn_output.shape:(batch_size,seq_len,d_model)\n",
    "        #out1.shape:(batch_size,seq_len,d_model)\n",
    "        attn_output,_=self.mha(x,x,x,encoder_padding_mask)\n",
    "        attn_output=self.dropout1(attn_output,training=training)\n",
    "        out1=self.layer_norml(x+attn_output)\n",
    "        \n",
    "        #ffn_output.shape:(batch_size,seq_len,d_model)\n",
    "        #out2.shape:(batch_size,seq_len,d_model)\n",
    "        ffn_output=self.ffn(out1)\n",
    "        ffn_output=self.dropout2(attn_output,training=training)\n",
    "        out2=self.layer_norml(out1+ffn_output)\n",
    "        \n",
    "        return out2\n",
    "        \n",
    "sample_encoder_layer=EncoderLayer(512,8,2048)\n",
    "sample_input=tf.random.uniform((64,50,512))\n",
    "sample_output=sample_encoder_layer(sample_input,False,None)\n",
    "print(sample_output.shape) #经过encoder后x的shape没有变\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self,num_layers,input_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "        super(EncoderModel,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_layers=num_layers\n",
    "        self.max_length=max_length\n",
    "        \n",
    "        self.embedding=keras.layers.Embedding(input_vocab_size,self.d_model)\n",
    "        #position_embedding.shape:(1,max_length,d_mdoel)\n",
    "        self.position_embedding=get_positional_embedding(max_length,self.d_model)\n",
    "        self.dropout=keras.layers.Dropout(rate)\n",
    "        self.encoder_layers=[EncoderLayer(d_model,num_heads,dff,rate)\n",
    "                             for _ in range(self.num_layers)]\n",
    "        \n",
    "    def call(self,x,training,encoder_padding_mask):\n",
    "        #x.shape:(batch_size,input_seq_len)\n",
    "        input_seq_len=tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal( input_seq_len,self.max_length,\" input_seq_len<=self.max_length\")\n",
    "        \n",
    "        #x.shape:(batch_size,input_seq_len,d_model)\n",
    "        x=self.embedding(x)\n",
    "        x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))#缩放，使x的作用更大些\n",
    "        x+=self.position_embedding[:,:input_seq_len,:]\n",
    "        x=self.dropout(x,training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x=self.encoder_layers[i](x,training,encoder_padding_mask)\n",
    "            \n",
    "        return x   \n",
    "        #x.shape:(batch_size,input_seq_len,d_modl)\n",
    "        \n",
    "sample_encoder_model=EncoderModel(2,8500,400,512,8,2048)\n",
    "\n",
    "sample_encoder_model_input=tf.random.uniform((64,37))\n",
    "sample_encoder_model_output=sample_encoder_model(sample_encoder_model_input,False,encoder_padding_mask=None)   \n",
    "print(sample_encoder_model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型把层当函数使用调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x->self.attention->add&normalize&dropout->out1\n",
    "    out1,encoding_outputs->attention->add&normalize&dropout->out2\n",
    "    out2->ffn->add&normalize&dropout->out3\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads,dff,rate=0.1):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.mha1=MultiHeadAttention(d_model,num_heads)\n",
    "        self.mha2=MultiHeadAttention(d_model,num_heads)\n",
    "        \n",
    "        self.ffn=feed_forward_network(d_model,dff)\n",
    "        \n",
    "        self.layer_normal=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norma2=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norma3=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.dropout1=keras.layers.Dropout(rate)\n",
    "        self.dropout2=keras.layers.Dropout(rate)\n",
    "        self.dropout3=keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self,x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask):\n",
    "        #decoder_mask:由look_ahead_mask和decoder_padding_mask合并而来\n",
    "        \n",
    "        #x.shape:(batch_size,target_seq_len,d_model)\n",
    "        #encoding_outputs.shape(batch_size,input_)\n",
    "        \n",
    "        #attn1,out1.shape:(batch_size,target_seq_len,d_model)\n",
    "        attn1,attn_weights1=self.mha1(x,x,x,decoder_mask)\n",
    "        attn1=self.dropout1(attn1,training=training)\n",
    "        out1=self.layer_normal(attn1+x)\n",
    "        \n",
    "        attn2,attn_weights2=self.mha2(\n",
    "        out1,encoding_outputs,encoding_outputs,encoder_decoder_padding_mask)\n",
    "        \n",
    "        attn2=self.dropout2(attn2,training=training)\n",
    "        out2=self.layer_norma2(attn2+out1)\n",
    "        \n",
    "        ffn_output=self.ffn(out2)\n",
    "        ffn_output=self.dropout3(ffn_output,training=training)\n",
    "        out3=self.layer_norma3(ffn_output+out2)\n",
    "        \n",
    "        return out3,attn_weights1,attn_weights2\n",
    "    \n",
    "sample_decoder_layer=DecoderLayer(512,8,2048)\n",
    "sample_decoder_input=tf.random.uniform((64,60,512))\n",
    "sample_decode_output,sample_decoder_attn_weights1,sample_decoder_attn_weights2=sample_decoder_layer(sample_decoder_input,sample_output,False,None,None)\n",
    "\n",
    "\n",
    "print(sample_decode_output.shape,sample_decoder_attn_weights1.shape,sample_decoder_attn_weights2.shape)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(keras.layers.Layer):\n",
    "        def __init__(self,num_layers,target_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "            super(DecoderModel,self).__init__()\n",
    "            self.d_model=d_model\n",
    "            self.num_layers=num_layers\n",
    "            self.max_length=max_length\n",
    "\n",
    "            self.embedding=keras.layers.Embedding(target_vocab_size,self.d_model)\n",
    "            #position_embedding.shape:(1,max_length,d_mdoel)\n",
    "            self.position_embedding=get_positional_embedding(max_length,self.d_model)\n",
    "            self.dropout=keras.layers.Dropout(rate)\n",
    "            self.decoder_layers=[DecoderLayer(d_model,num_heads,dff,rate)\n",
    "                                 for _ in range(self.num_layers)]\n",
    "        def call(self,x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask):\n",
    "            #x.shape:(batch_size,out_seq_len)\n",
    "            output_seq_len=tf.shape(x)[1]\n",
    "            tf.debugging.assert_less_equal(output_seq_len,self.max_length,\"output_seq_len<=self.max_length\") \n",
    "            \n",
    "            attention_weights={}\n",
    "            \n",
    "            x=self.embedding(x)\n",
    "            x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
    "            x+=self.position_embedding[:,:output_seq_len,:]\n",
    "            x=self.dropout(x,training=training)\n",
    "            \n",
    "            for i in range(self.num_layers):\n",
    "                x,attn1,attn2=self.decoder_layers[i](x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask)\n",
    "                attention_weights['decoder{}_att1'.format(i+1)]=attn1\n",
    "                attention_weights['decoder{}_att2'.format(i+1)]=attn2\n",
    "                                  \n",
    "            #x.shape:(batch_size,out_seq_len,d_,model)\n",
    "            return x,attention_weights\n",
    "                                  \n",
    "sample_decoder_model  =DecoderModel(2,8000,1000,512,8,2048)                                \n",
    "                                  \n",
    "sample_decoder_model_input=tf.random.uniform((64,35))\n",
    "sample_decoder_model_output,sample_decoder_model_att=sample_decoder_model(\n",
    "                    sample_decoder_model_input,\n",
    "                   sample_encoder_model_output,\n",
    "training=False,decoder_mask=None,encoder_decoder_padding_mask=None)\n",
    "                                  \n",
    "print(sample_decoder_model_output.shape)\n",
    "for key in sample_decoder_model_att:\n",
    "    print(sample_decoder_model_att[key].shape)#DecoderModel3层，所以attn有4个，35,35是self-attn，35,37是encoder-decoder atten\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(self,num_layers,input_vocab_size,target_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.encoder_model=EncoderModel(num_layers,input_vocab_size,max_length\n",
    "                                       ,d_model,num_heads,dff,rate)\n",
    "        \n",
    "        self.decoder_model=DecoderModel(num_layers,input_vocab_size,max_length\n",
    "                                       ,d_model,num_heads,dff,rate)\n",
    "        self.final_layer=keras.layers.Dense(target_vocab_size)\n",
    "    def call(self,inp,tar,training,encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask):\n",
    "        \n",
    "        #encoding_outputs.shape:(batch_size,input-seq_len,d_model)\n",
    "        encoding_outputs=self.encoder_model(inp,training,encoder_padding_mask)\n",
    "        #decoding_outputs.shape:(batch_size,output_seq_len,target_vocab_\n",
    "        decoding_outputs,attention_weights=self.decoder_model(\n",
    "                    tar,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask)\n",
    "        \n",
    "        predictions=self.final_layer(decoding_outputs)\n",
    "        return predictions,attention_weights\n",
    "    \n",
    "sample_transformer=Transformer(2,8500,8000,1000,512,8,2048,rate=0.1)\n",
    "\n",
    "temp_input=tf.random.uniform((64,26))\n",
    "temp_target =tf.random.uniform((64,31))\n",
    "predictions,attention_weights=sample_transformer(temp_input,temp_target,\n",
    "                   training=False,\n",
    "                   encoder_padding_mask=None,\n",
    "                   decoder_mask=None,encoder_decoder_padding_mask=None)\n",
    "\n",
    "print(predictions.shape)\n",
    "for key in attention_weights:\n",
    "    print(key,attention_weights[key].shape)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.initializes model\n",
    "#2.define loss ,optimizer, learning_rate_schedule\n",
    "#3.train_step\n",
    "#4.train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=4\n",
    "d_model=128\n",
    "dff=512\n",
    "num_heads=8\n",
    "input_vocab_size=pk_tokenizer.vocab_size+2#因为要加start和end\n",
    "target_vocab_size=en_tokenizer.vocab_size+2\n",
    "dropout_rate=0.1\n",
    "transformer=Transformer(num_layers,\n",
    "                       input_vocab_size,\n",
    "                       target_vocab_size,\n",
    "                       max_length,\n",
    "                       d_model,num_heads,\n",
    "                       dff,\n",
    "                       dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrate=(d_model**-0.5)*min(step_num**(-0.5),\n",
    "#                  step_num*warm_up_steps**(-1.5) )\n",
    "\n",
    "class CustomizedSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,d_model,warmup_steps=4000):\n",
    "        super(CustomizedSchedule,self).__init__()\n",
    "        self.d_model=tf.cast(d_model,tf.float32)\n",
    "        self.warmup_steps=warmup_steps\n",
    "    \n",
    "    def __call__(self,step):\n",
    "        arg1=tf.math.rsqrt(step)\n",
    "        arg2=step*(self.warmup_steps**(-1.5))\n",
    "        \n",
    "        arg3=tf.math.rsqrt(self.d_model)\n",
    "        \n",
    "        return arg3*tf.math.minimum(arg1,arg2)\n",
    "learning_rate=CustomizedSchedule(d_model)\n",
    "optimizer=keras.optimizers.Adam(learning_rate,beta_1=0.9,beta_2=0.98,epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule=CustomizedSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000,dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Train step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object=keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True,reduction='none')#因为要对mask进行处理，不能用本身自带的reduction，所以要为none\n",
    "def loss_function(real,pred):\n",
    "    mask=tf.math.logical_not(tf.math.equal(real,0))#有padding的地方mask值都为零\n",
    "    loss_=loss_object(real,pred)\n",
    "    mask=tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_*=mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp,tar):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "        -encoder_padding_mask 计算attention时候的(self attention of EncoderLayer)\n",
    "    Decoder:\n",
    "        -look_ahead_mask 前面单词不能看到后面的(self attention of DecoderLayer)\n",
    "        -encoder_decoder_padding_mask  encoder_decoder之间的mask(encoder-decoder attention)\n",
    "        -decoder_padding_mask decoder上的attention (self attention of DecoderLayer)\n",
    "    \"\"\"\n",
    "    encoder_padding_mask=create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask=create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask=create_look_ahead_mask(tf.shape(tar)[1])#被mask是1，不被mask是零\n",
    "    decoder_padding_mask=create_padding_mask(tar)\n",
    "    decoder_mask=tf.maximum(decoder_padding_mask,look_ahead_mask)\n",
    "    print(encoder_padding_mask.shape)\n",
    "    print(encoder_decoder_padding_mask.shape)\n",
    "    print(look_ahead_mask.shape)#正方形矩阵，上三角是1，下三角是0\n",
    "    print(decoder_padding_mask.shape)\n",
    "    print(decoder_mask.shape)#tensor的自动补全机制，补上2个维度，把下一个维度复制39份\n",
    "    return encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inp,temp_tar=iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=keras.metrics.Mean(name='train_loss')#定义累计的平均loss\n",
    "train_accuracy=keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp,tar):\n",
    "    tar_inp=tar[:,:-1]\n",
    "    tar_rel=tar[:,1:]\n",
    "    \n",
    "    encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask=\\\n",
    "                                create_masks(inp,tar_inp)\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions,_=transformer(inp,tar_inp,True,encoder_padding_mask,\n",
    "                                 decoder_mask,\n",
    "                                 encoder_decoder_padding_mask)\n",
    "        \n",
    "        loss=loss_function(tar_real,predictions)\n",
    "        \n",
    "    gradients=tape.gradient(loss,transformer.trainable_variables)#计算梯度\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))#将梯度绑定到变量上\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real,preictions)#计算累计 的accuracy\n",
    "    \n",
    "    \n",
    "epochs=20\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss.reset_states()#从零开始累计\n",
    "    train_accuracy.reset_states()\n",
    "    for (batch,(inp,tar)) in enumerate(train_dataset):\n",
    "        train_step(inp,tar)\n",
    "        \n",
    "        if batch%100==0:\n",
    "            print('Epoch{} Batch{} Loss{:.4f} Accuracy{:.4f}'.format(epoch+1,batch,train_losss.result(),\n",
    "                                                                    train_accuracy.result())\n",
    "                         )\n",
    "            \n",
    "    \n",
    "    \n",
    "    print('Epoch{}Loss{:.4f} Accuracy {:.4f}'.format(\n",
    "                    epoch+1,train_loss.result(),train_accuracy.result()))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eg:A B C D->E F G H.\n",
    "train: A B C D,E F G->F G H\n",
    "Eval:A B C D ->E\n",
    "    A B C D E->F\n",
    "    A B C D E F ->G\n",
    "    A B C D E F G->H    transform训练时并行处理，预测时候不是\n",
    "\"\"\"\n",
    "def evaluate(inp_sentence):\n",
    "    input_id_sentence=[pt_tokenizer.vacab_size]+pt_tokenizer.encode(inp_sentence)+[pt_tokenizer.vacab_size+1]#把文本的句子转化成id的句子\n",
    "    #encoder_input.shape:(1,input_sentence_length)\n",
    "    encoder_input=tf.expand_dims(input_id_sentence,0)\n",
    "    #decoder_input.shape:(1,1)\n",
    "    decoder_input=tf.expand_dims([en_tokenizer.vocab_size],0)\n",
    "    #transform 与seqtoseq不一样：transform中decoder_input为多少，就会给出多少个预测值（多步预测）。seqtoseq天生是单步预测，每次只得到一个值。\n",
    "    for i in range(max_length):\n",
    "        encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask\\\n",
    "        =create_masks(encoder_input,decoder_input)\n",
    "        #predictions.shape:(batch_size,output_target_len,target_vocab_size)\n",
    "        predictions,attention_weights=transformer(encoder_input,decoder_input,False,\n",
    "                                                  encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask)\n",
    "        \n",
    "        #predictions.shape:(batch_size,target_vocab_size),中间维度只去一个值，维度消失\n",
    "        predictions=predictions[:,-1,:]#只需要最后一个预测值，就是decoder_nput全部输入进去得到的值\n",
    "        predicted_id=tf.cast(tf.argmax(predictions,axis=-1),tf.int32)\n",
    "        \n",
    "        if tf.equal(predicted_id,en_tokenizer.vocab_size+1):\n",
    "            return tf.squeeze(decoder_input,axis=0),attention_weights\n",
    "        \n",
    "        decoder_input=tf.concat([decoder_input,predicted_id],axis=-1)\n",
    "        \n",
    "    return tf.squeeze(decoder_input,axis=0),attention_weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units=256\n",
    "units=1024\n",
    "input_vocab_size=len(input_tokenizer.word_index)+1#??\n",
    "output_vocab_size=len(output_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_units,encoding_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.encoding_units=encoding_units\n",
    "        self.embedding=keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru=keras.layers.GRU(self.encoding_units,return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform')    \n",
    "    def call(self,x,hidden):\n",
    "        x=self.embedding(x)\n",
    "        output,state=self.gru(x,initial_state=hidden)\n",
    "        return output,state #每一步的输出，以及最后一步输出的隐含状态      \n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units))\n",
    "    \n",
    "encoder=Encoder(input_vocab_size,embedding_units,units,batch_size)\n",
    "sample_hidden=encoder.initialize_hidden_state()\n",
    "sample_output,sample_hidden=encoder(x,sample_hidden)\n",
    "print(sample_output.shape)#输出的1024是状态是size\n",
    "print(sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.w1=keras.layers.Dense(units)\n",
    "        self.w2=keras.layers.Dense(units)\n",
    "        self.v=keras.layers.Dense(1)\n",
    "        \n",
    "        \n",
    "    def call(self,decoder_hidden,encoder_outputs):\n",
    "        #decoder_hidden.shape:(batch_size,units)\n",
    "        #encoder_outputs.shape:(batch_size,length,units)\n",
    "        decoder_hidden_with_time_axis=tf.expand_dims(decoder_hidden,1)\n",
    "        \n",
    "        #before v:(batch_size,length,units)\n",
    "        #after v:(batch_size,units)\n",
    "        score=self.v(\n",
    "            tf.nn.tanh(self.w1(encoder_outputs)+self.w2(decoder_hidden_with_time_axis)))\n",
    "        #shape:(batch_size,length,1)\n",
    "        attention_weights=tf.nn.softmax(score,axis=1)\n",
    "        #context_vector.shape:(batch_size,length,units)在最后一个维度做扩展了\n",
    "        context_vector=attention_weights*encoder_outputs\n",
    "        #context_vector.shape(batch_size,units)在length上求和，length维度就没有了？？\n",
    "        context_vector=tf.reduce_sum(context_vector,axis=1)\n",
    "        \n",
    "        return context_vector,attention_weights\n",
    "    \n",
    "attention_model=BahdanauAttention(units=10)\n",
    "attention_results,attention_weights=attention_model(sample_hidden,sample_output)\n",
    "\n",
    "print(attention_results.shape,attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_units,decoding_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.decoding_units=decoding_units\n",
    "        self.embedding=keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru=keras.layers.GRU(self.decoding_units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc=keras.layers.Dense(vocab_size)\n",
    "        self.attention=BahdanauAttention(self.decoding_units)\n",
    "        \n",
    "        \n",
    "    def call(self,x,hidden,encoding_outputs):\n",
    "        #context_vector.shape:(batch_size,units)\n",
    "        context_vector,attention_weights=self.attention(hidden,encoding_outputs)\n",
    "        #befor embedding:x.shape:(batch,1)decoding是单步的decoding\n",
    "        #after embedding:x.shape:(batch_size,1,embedding_units)\n",
    "        x=self.embedding(x)\n",
    "        \n",
    "        combined_x=tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)#在最后一个维度上拼接\n",
    "        \n",
    "        #output.shape:[batch_size,1,decoding_units]\n",
    "        #state.shape:[batch_size,decoding_units]\n",
    "        \n",
    "        output,state=self.gru(combined_x)\n",
    "        #output.shape:[batch_size,decoding_units]\n",
    "        output=tf.reshape(output,(-1,output.shape[2]))\n",
    "        \n",
    "        #output.shape:[batch_size,vocab_size]\n",
    "        output=self.fc(output)\n",
    "        \n",
    "        return output,state,attention_weights\n",
    "        \n",
    "        \n",
    "decoder=Decoder(output_vocab_size,embedding_units,units,batch_size)\n",
    "\n",
    "outputs=decode(tf.random.uniform((batch_size,1)),sample_hidden,sample_output)\n",
    "decode_output,decoder_hidden,decoder_aw=outputs\n",
    "\n",
    "print(decode_output.shape,decoder_hidden.shape,decoder_aw.shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer=keras.optimizers.Adam()\n",
    "\n",
    "loss_object=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')# reduction表示损失函数该如何聚                                                     \n",
    "        \n",
    "def loss_function(real,pred):\n",
    "    mask= tf.math.logical_not(tf.math.equal(real,0))# tf.math.equal(real,0) 是0的返回True，目标是让mask中不是零的变成1，其余为0          #mask输出的时候有很多padding，padding不应该计算在损失函数中去，把padding对应的损失函数对应的改为零\n",
    "    loss_=loss_object(real,pred)\n",
    "    mask=tf.cast(mask,dtype=loss_.dtype)#把mask布尔数据类型，转化成loss相同的\n",
    "    \n",
    "    loss_*=mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)# reduction='None'先不具合，乘完以后再聚合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp,targ,encoding_hidden):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs,encoding_hidden=encoder(inp,encoding_hidden)\n",
    "        decoding_hidden=encoding_hidden\n",
    "        for t in range(0,targ.shape[1]-1):\n",
    "            decoding_input=tf.expand_dims(targ[:,t],1)\n",
    "            predictions,decoding_hidden,_=decoder(decoding_input,decoding_hidden,\n",
    "                                                 encoding_outputs)\n",
    "            loss+=loss_function(targ[:,t+1],predictions)\n",
    "            \n",
    "    batch_loss=loss/int(targ.shape[0])\n",
    "    variables=encoder.trainable_variables+decoder.trainable_variables\n",
    "    gradients=tape.gradient(loss,variables)\n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "    \n",
    "    return batch_loss\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10#循环遍历10次数据集\n",
    "steps_per_epoch=len(input_tensor)//batch_size#遍历数据集一次\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    encoding_hidden=encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss=0\n",
    "    \n",
    "    for (batch,(inp,targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss=train_step(inp,targ,encoding_hidden)\n",
    "        total_loss+=batch_loss\n",
    "        \n",
    "        if batch%100==0:\n",
    "            print('Epoch{} Batch {} Loss {:.4f}'.format(epoch+1,batch,batch_loss.numpy()))\n",
    "        \n",
    "    \n",
    "    print('Epoch{} Batch {} Loss {:.4f}'.format(epoch+1,batch,batch_loss.numpy()))\n",
    "    print('Time take for 1 epoch {} sec'.format(time.time()-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<END>'] = 3\n",
    "\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for key, value in word_index.items()])\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join(\n",
    "        [reverse_word_index.get(word_id, \"<UNK>\") for word_id in text_ids])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix=np.zeros((max_length_output,max_length_input))\n",
    "    input_sentence=preprocess_sentence(input_sentence)\n",
    "    inputs=[input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs=keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs],maxlen=max_length_input,padding='post'\n",
    "    )\n",
    "    inputs=tf.convert_to_tensor(inputs)#保存成tensor\n",
    "    \n",
    "    results=''\n",
    "#     encoding_hidden=encoder.initialize_hidden_state()\n",
    "    encoding_hidden=tf.zeros((1,units))    \n",
    "    encoding_outputs,encoding_hidden=encoder(inputs,encoding_hidden)\n",
    "    decoding_hidden=encoding_hidden\n",
    "    \n",
    "    #eg:<start>->A\n",
    "    #A->B->C->D\n",
    "    #decoding_input.shape:(1,1)\n",
    "    decoding_input=tf.expand_dims(output_tokenizer.word_index['<start>'],0)\n",
    "    for t in range(max_length_output):\n",
    "        pedictions,decoding_hidden,attention_weights=decoder(decoding_input,decoder_hidden,encoding_outputs)\n",
    "        \n",
    "        #attention_weights.shape:(batch_size,input_length,1)(1,16,1)\n",
    "        attention_weights=tf.reshape(attention_weights,(-1,))#得到长度为16的向量？\n",
    "        attention_matrix[t]=attention_matrix.numpy()\n",
    "        \n",
    "        #predictions.shape:(batch_size,vocab_size)(1,4935)\n",
    "        predicted_id=tf.argmax(predictions[0].numpy())\n",
    "        \n",
    "        results+=output_tokenizer.index_word[predicted_id]+' '\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id]=='<end>':\n",
    "            return results,input_sentence,attention_matrix\n",
    "            \n",
    "        \n",
    "        decoding_input=tf.expand_dims([predicted_id],0)\n",
    "        \n",
    "    return results,input_sentence,attention_matrix\n",
    " \n",
    "\n",
    "def plot_attention(attention_matrix,input_sentence,predicted_sentence):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    ax=fig,add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix,cmap='viridis')\n",
    "    font_dict={'fontsize':14}\n",
    "    \n",
    "    ax.set_xticklabels(['']+input_sentence,font_dict=font_dict,rotation=90)\n",
    "    ax.set_yticklabels(['']+predicted_sentence,fontdict=font_dict)\n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results,input_sentence,attention_matrix=evaluate(input_sentence)\n",
    "    \n",
    "    print(input_sentence)\n",
    "    print(results)\n",
    "    \n",
    "    attention_matrix=attention_matrix[:len(results.split(\" \")),:len(input_sentence.split(' '))]\n",
    "    \n",
    "    plot_attention(attention_matrix,input_sentence.split(' '),results.split(\" \"))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Te amo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data, # list of list\n",
    "    value = word_index['<PAD>'],\n",
    "    padding = 'post', # post, pre\n",
    "    maxlen = max_length)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data, # list of list\n",
    "    value = word_index['<PAD>'],\n",
    "    padding = 'post', # post, pre\n",
    "    maxlen = max_length)\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "single_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.LSTM(units = 64, return_sequences = False),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "single_rnn_model.summary()\n",
    "single_rnn_model.compile(optimizer = 'adam',\n",
    "                         loss = 'binary_crossentropy',\n",
    "                         metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_single_rnn = single_rnn_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 30,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history_single_rnn, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history_single_rnn, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_rnn_model.evaluate(\n",
    "    test_data, test_labels,\n",
    "    batch_size = batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 64, return_sequences = True)),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 64, return_sequences = False)),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 30,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "bi_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 32, return_sequences = False)),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "bi_rnn_model.summary()\n",
    "bi_rnn_model.compile(optimizer = 'adam',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bi_rnn_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 5,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn_model.evaluate(test_data, test_labels, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
