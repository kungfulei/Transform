{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "sys.version_info(major=3, minor=7, micro=2, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.2\n",
      "pandas 1.0.3\n",
      "sklearn 0.22.2.post1\n",
      "tensorflow 2.1.0\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.loads data\n",
    "#.preprocess data->dataset\n",
    "#3.tools\n",
    "#3.1 generates position embedding\n",
    "#3.2 create mask.(a.padding,b.decoder)\n",
    "#3.3 scaled_dot_product_attention\n",
    "#4.builds model\n",
    "#4.1 MultiheadAttention\n",
    "#4.2 EncoderLayer\n",
    "#4.3 DecoderLayer\n",
    "#4.4 EncoderModel\n",
    "#4.5 DecoderModel\n",
    "#4.6 Transformer\n",
    "#5.optimizer &loss\n",
    "#6.train step ->train\n",
    "#7.Evaluate and Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spa_file_path='./spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in  unicodedata.normalize('NFD',s) if unicodedata.category(c) !='Mn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s=unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    #标点符号前后加空格\n",
    "    s=re.sub(r\"([?.!,])\",r\" \\1 \",s)\n",
    "    \n",
    "    #多余的空格变成一个空格\n",
    "    s=re.sub(r'[\" \"]+',\" \",s)\n",
    "    #去掉前后空格\n",
    "    s=s.rstrip().strip()\n",
    "    s='<start> '+s+ ' <end>'\n",
    "    return s\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines=open(filename,encoding='utf-8').read().strip().split('\\n')\n",
    "    sentence_pairs=[line.split('\\t') for line in lines]\n",
    "   \n",
    "    preprocessed_sentence_pairs=[ \n",
    "        (preprocess_sentence(en),preprocess_sentence(sp) )for en,sp in sentence_pairs]\n",
    "    \n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en_dataset,sp_dataset=parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 432, 3, 2]\n",
      "[  1 709   3   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[1, 31, 3, 2]\n",
      "[ 1 31  3  2  0  0  0  0  0  0  0]\n",
      "16 11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer=keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None,filters='',split=' ')# num_words指定多少单词\n",
    "    lang_tokenizer.fit_on_texts(lang)#根据词频生成词表\n",
    "    tensor=lang_tokenizer.texts_to_sequences(lang)#生成句子id表示\n",
    "    print(tensor[1])\n",
    "    tensor=keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    print(tensor[2])\n",
    "    return tensor,lang_tokenizer\n",
    "\n",
    "input_tensor,input_tokenizer=tokenizer(sp_dataset[0:30000])\n",
    "output_tensor,output_tokenizer=tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input=max_length(input_tensor)\n",
    "max_length_output=max_length(output_tensor)\n",
    "print(max_length_input,max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 6000 24000 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train ,input_eval,output_train,output_eval=train_test_split(input_tensor,output_tensor,test_size=0.2)\n",
    "print(len(input_train),len(input_eval),len(output_train),len(output_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1--><start>\n",
      "8-->el\n",
      "272-->mundo\n",
      "6-->es\n",
      "499-->pequeno\n",
      "3-->.\n",
      "2--><end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example,tokenizer):\n",
    "    for t in example:\n",
    "        if t !=0:\n",
    "            print(\"{}-->{}\".format(t,tokenizer.index_word[t]))\n",
    "            \n",
    "convert(input_train[5],input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor,output_tensor,batch_size,epochs,shuffle):\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((input_tensor,output_tensor))\n",
    "    if shuffle:\n",
    "        dataset=dataset.shuffle(30000)\n",
    "    dataset=dataset.repeat(epochs).batch(batch_size,drop_remainder=True)\n",
    "    return dataset\n",
    "    \n",
    "batch_size=64\n",
    "epochs=20\n",
    "    \n",
    "train_dataset=make_dataset(input_train,output_train,batch_size,epochs,True)\n",
    "eval_dataset=make_dataset(input_eval,output_eval,batch_size,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[   1 2746   10 ...    0    0    0]\n",
      " [   1    5 9896 ...    0    0    0]\n",
      " [   1   22  274 ...    0    0    0]\n",
      " ...\n",
      " [   1   32  903 ...    0    0    0]\n",
      " [   1    7   25 ...    0    0    0]\n",
      " [   1    7  231 ...    0    0    0]], shape=(64, 16), dtype=int32) tf.Tensor(\n",
      "[[   1  291   10  840    3    2    0    0    0    0    0]\n",
      " [   1    5  315  666  125    3    2    0    0    0    0]\n",
      " [   1   25  243   28 2803    3    2    0    0    0    0]\n",
      " [   1   25    8 1576    3    2    0    0    0    0    0]\n",
      " [   1   13  430  452    3    2    0    0    0    0    0]\n",
      " [   1  342   47   46   54    3    2    0    0    0    0]\n",
      " [   1   63  272  322    3    2    0    0    0    0    0]\n",
      " [   1   33  332  132    3    2    0    0    0    0    0]\n",
      " [   1  278    7  230    6    2    0    0    0    0    0]\n",
      " [   1   35  138   21  437    6    2    0    0    0    0]\n",
      " [   1  277   57  130  703    3    2    0    0    0    0]\n",
      " [   1   18    7   23    9  449    6    2    0    0    0]\n",
      " [   1   15   79  250    3    2    0    0    0    0    0]\n",
      " [   1    4  624   50    7    3    2    0    0    0    0]\n",
      " [   1   13  930   46   62   73   34    2    0    0    0]\n",
      " [   1   13    8  106    3    2    0    0    0    0    0]\n",
      " [   1    5  186  362    3    2    0    0    0    0    0]\n",
      " [   1   55  979    3    2    0    0    0    0    0    0]\n",
      " [   1  475   14    9 4062    3    2    0    0    0    0]\n",
      " [   1   37  642 1032    3    2    0    0    0    0    0]\n",
      " [   1  113   10  209  193    3    2    0    0    0    0]\n",
      " [   1   11 1046   50 1108    3    2    0    0    0    0]\n",
      " [   1    4  168   10 2679    3    2    0    0    0    0]\n",
      " [   1   21   19 1168    3    2    0    0    0    0    0]\n",
      " [   1   21   61    9 4152    3    2    0    0    0    0]\n",
      " [   1 1840    8 2183  424 3624    3    2    0    0    0]\n",
      " [   1   15   45 1216    3    2    0    0    0    0    0]\n",
      " [   1    5  206   35   21   40    3    2    0    0    0]\n",
      " [   1   52   36   71    3    2    0    0    0    0    0]\n",
      " [   1   22  369    8  204    3    2    0    0    0    0]\n",
      " [   1    5   48  103  407    3    2    0    0    0    0]\n",
      " [   1    4  182 1504  154  312    3    2    0    0    0]\n",
      " [   1   19    7  779   38    6    2    0    0    0    0]\n",
      " [   1    4  139   22 2248    3    2    0    0    0    0]\n",
      " [   1    5    8   45 1370    3    2    0    0    0    0]\n",
      " [   1   86 1253   53   14    3    2    0    0    0    0]\n",
      " [   1   13  743   10 3023    3    2    0    0    0    0]\n",
      " [   1    5 1675   44    3    2    0    0    0    0    0]\n",
      " [   1   23    7  700    6    2    0    0    0    0    0]\n",
      " [   1   27   45  634    3    2    0    0    0    0    0]\n",
      " [   1   25  781 1434    3    2    0    0    0    0    0]\n",
      " [   1   15 2693    7    3    2    0    0    0    0    0]\n",
      " [   1    5  899   10  450    3    2    0    0    0    0]\n",
      " [   1   10  109    8  204    3    2    0    0    0    0]\n",
      " [   1    4  303   26  396    3    2    0    0    0    0]\n",
      " [   1    5  471   44  150    3    2    0    0    0    0]\n",
      " [   1   76   62  266    3    2    0    0    0    0    0]\n",
      " [   1    4  181    5    9 4850    3    2    0    0    0]\n",
      " [   1    4  248   37  393    3    2    0    0    0    0]\n",
      " [   1   25  245   12 4320    3    2    0    0    0    0]\n",
      " [   1    4   18   77    7    3    2    0    0    0    0]\n",
      " [   1   13   74   31  228  114    3    2    0    0    0]\n",
      " [   1   10  220  775  468    3    2    0    0    0    0]\n",
      " [   1  163  146  599   34    2    0    0    0    0    0]\n",
      " [   1    4   32   18   11  159    3    2    0    0    0]\n",
      " [   1   27   79 1270    3    2    0    0    0    0    0]\n",
      " [   1   11    8   79  566   50   14    3    2    0    0]\n",
      " [   1   55  154    3    2    0    0    0    0    0    0]\n",
      " [   1   18    7   30  367    6    2    0    0    0    0]\n",
      " [   1    4   32   49   96    3    2    0    0    0    0]\n",
      " [   1   35 2129   11    6    2    0    0    0    0    0]\n",
      " [   1   10  938   19  814    3    2    0    0    0    0]\n",
      " [   1    4   24   43    7   38    3    2    0    0    0]\n",
      " [   1   24  705  123   82    3    2    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "#PE(pos,2i)=sin(pos/10000^(2i/d_modek))\n",
    "#PE(pos,2i+1)=cos(pos/10000^(2i/d_modek))\n",
    "\n",
    "#pos:[sententce_length,1]\n",
    "#i.shape :[1,d_model]\n",
    "#resule.shape:[sententce_length,d_model]\n",
    "def get_angles(pos,i,d_model):#pos词语在句子中的位置，i是在embedding中的位置,d_model是embedding的大小\n",
    "    angle_rates=1/np.power(10000,(2*(i//2))/np.float32(d_model))       \n",
    "    return pos*angle_rates\n",
    "def get_positional_embedding(sentence_length,d_model):\n",
    "    angle_rads=get_angles(np.arange(sentence_length)[:,np.newaxis],\n",
    "                         np.arange(d_model)[np.newaxis,:],d_model)#np.arange(sentence_length)[:,np.newaxis]绛sentence——length向量扩展成矩阵\n",
    "    #sines.shape:[sentence_length,d_model/2]\n",
    "    #cosine.shape:[sentence_length,d_model/2]\n",
    "    sines=np.sin(angle_rads[:,0::2])\n",
    "    cosines=np.cos(angle_rads[:,1::2])   \n",
    "    #position_embedding.shape:[sentence_length,d_model]\n",
    "    position_embedding=np.concatenate([sines,cosines],axis=-1)\n",
    "    #position_embedding.shape:[1,sentence_length,d_model]\n",
    "    position_embedding=position_embedding[np.newaxis,...]\n",
    "    \n",
    "    return tf.cast(position_embedding,dtype=tf.float32)#转化类型\n",
    "position_embedding=get_positional_embedding(50,512)\n",
    "\n",
    "print(position_embedding.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_position_embedding(position_embedding):\n",
    "    plt.pcolormesh(position_embedding[0],cmap='RdBu')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0,512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "plot_position_embedding(position_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1., 0., 1., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.padding mask,2.look ahead(decoder只能和之前的词语发生关系，不能和之后的词语发生关系)\n",
    "\n",
    "#batch_data.shape:[batch_size,seq_len]\n",
    "\n",
    "def create_padding_mask(batch_data):\n",
    "    \n",
    "    padding_mask=tf.cast(tf.math.equal(batch_data,0),tf.float32)\n",
    "    #[batch_size,1,1,seq_len]\n",
    "    return padding_mask[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "x=tf.constant([[4,45,3,0,0],[5,0,34,3,0],[89,0,3,0,1]])\n",
    "create_padding_mask(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attention_weights.shape:[3,3]\n",
    "#[[1,0,0],1表示第一个单词与本身的关系，其余类推\n",
    "#[4,4,0],\n",
    "#[3,5,4]]\n",
    "def create_look_ahead_mask(size):\n",
    "    mask=1-tf.linalg.band_part(tf.ones((size,size)),-1,0)\n",
    "    return mask\n",
    "\n",
    "create_look_ahead_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q:shape==(...,seq_len_q,depth)  q和k要做乘法，最后一个维度要相等\n",
    "        k:shape==(...,seq_len_k,depth)\n",
    "        v:shape==(...,seq_len_v,depth_v) \n",
    "        seq_len_k ==seq_len_v\n",
    "        mask:shape==(...,seq_len_q,seq_len_k)\n",
    "    \"\"\"\n",
    "    matmul_qk=tf.matmul(q,k,transpose_b=True)\n",
    "    dk=tf.cast(tf.shape(k)[-1],tf.float32)#dk 是k最后一维的维度\n",
    "    scaled_attention_logits=matmul_qk/tf.math.sqrt(dk)   \n",
    "    if mask is not None:\n",
    "        #使得在softmax后值趋近于0\n",
    "        scaled_attention_logits+=(mask * -1e9)#为什么要用加法而不是乘法呢？logits以后做softmax时候，由于加上-le9，会让他的值无线接近于零\n",
    "    #attention_weights.shape:(...,seq_len_q,seq_len_k)\n",
    "    attention_weights=tf.nn.softmax(scaled_attention_logits,axis=-1)#在最后一个维度做softmax\n",
    "    #output.shape:(...,seq_len_q,depth_v)\n",
    "    output=tf.matmul(attention_weights,v)#矩阵乘法只作用于后两位，前面维度忽略掉\n",
    "    return output,attention_weights\n",
    "def print_scaled_dot_product_attention(q,k,v):\n",
    "    temp_out,temp_att=scaled_dot_product_attention(q,k,v,None)\n",
    "    print('temp_out',temp_out)\n",
    "    print('``````')\n",
    "    print('temp_att',temp_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_out tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n",
      "``````\n",
      "temp_att tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_k=tf.constant([[10,0,0],\n",
    "                   [0,10,0],\n",
    "                   [0,0,10],\n",
    "                   [0,0,10]],dtype=tf.float32)\n",
    "temp_v=tf.constant([[1,0],\n",
    "                   [10,0],\n",
    "                   [100,5],\n",
    "                   [1000,6]],dtype=tf.float32)\n",
    "temp_q1=tf.constant([[0,10,0]],dtype=tf.float32)\n",
    "np.set_printoptions(suppress=True)#改变显示结果\n",
    "print_scaled_dot_product_attention(temp_q1,temp_k,temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512)\n",
      "(1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上：\n",
    "    x->Wq0->q0\n",
    "    x->Wk0->k0\n",
    "    x->Wv0->v0   \n",
    "    实战中：\n",
    "    q->Wq0->q0\n",
    "    k->Wk0->k0\n",
    "    v->Wv0->v0\n",
    "    实战中技巧;\n",
    "    q->wq->Q->split->q0,q1,q2  \n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.d_model=d_model\n",
    "        assert d_model % self.num_heads==0\n",
    "        self.depth=d_model//self.num_heads\n",
    "        self.WQ=keras.layers.Dense(self.d_model)\n",
    "        self.WK=keras.layers.Dense(self.d_model)\n",
    "        self.WV=keras.layers.Dense(self.d_model)\n",
    "        self.dense=keras.layers.Dense(self.d_model)\n",
    "    def split_heads(self,x,batch_size):\n",
    "        #x.shape:(batch_size,seq_len,d_model)\n",
    "        #d_mdoel=num_heads*depth\n",
    "        #x->(batch_size,num_heads,seq_len,depth)\n",
    "        x=tf.reshape(x,(batch_size,-1,self.num_heads,self.depth))#d_model->num_heads,depth\n",
    "        return tf.transpose(x,perm=[0,2,1,3])\n",
    "    def call(self,q,k,v,mask):\n",
    "        batch_size=tf.shape(q)[0]#???\n",
    "        #\n",
    "        q=self.WQ(q)#q.shape:(batch_size,seq_len_q,d_model)\n",
    "        k=self.WK(k)#k.shape:(batch_size,seq_len_k,d_model)\n",
    "        v=self.WV(v)#v.shape:(batch_size,seq_len_v,d_model)\n",
    "        #q.shape:(batch_size,num_heads,seq_len_q,depth)\n",
    "        q=self.split_heads(q,batch_size)\n",
    "        k=self.split_heads(k,batch_size)\n",
    "        v=self.split_heads(v,batch_size)\n",
    "        \n",
    "        #scaled_dot_product_attention.shape:(batch_size,num_heads,seq_len_q,depth)多头信息存在第二维和第4维上\n",
    "        #attention_wights.shape:(batch_size,num_heads,seq_len_q,seq_len_k)\n",
    "        scaled_attention_outputs,attention_weights=scaled_dot_product_attention(q,k,v,mask)\n",
    "        #scaled_attention_outputs.shape:(batch_size,seq_len_q,num_heads,depth)\n",
    "        scaled_attention_outputs=tf.transpose(scaled_attention_outputs,perm=[0,2,1,3])\n",
    "        #concat_attention.shape:(batch_size,seq_len_q,d_model)\n",
    "        concat_attention=tf.reshape(scaled_attention_outputs,(batch_size,-1,self.d_model))\n",
    "        #output.shape:(batch_size,seq_len_q,d_model)\n",
    "        output=self.dense(concat_attention)\n",
    "        return output,attention_weights\n",
    "        \n",
    "temp_mpa=MultiHeadAttention(d_model=512,num_heads=8)\n",
    "y=tf.random.uniform((1,60,256))#(batch_size,seq_len_q,dim)\n",
    "output,attn=temp_mpa(y,y,y,mask=None)\n",
    "print(output.shape)\n",
    "print(attn.shape)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(d_model,dff):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(dff,activation='relu'),\n",
    "        keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x->self attention ->add &normalize&dropout\\\n",
    "    ->feed_forward->add&normalize &dropout\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads,dff,rate=0.1):#rate,dropout\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha=MultiHeadAttention(d_model,num_heads)\n",
    "        self.ffn=feed_forward_network(d_model,dff)\n",
    "        \n",
    "        self.layer_norml=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norml=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.dropout1=keras.layers.Dropout(rate)\n",
    "        self.dropout2=keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self,x,training,encoder_padding_mask):\n",
    "        #x.shape:(batch_size,seq_len,dim=d_model输出结果一样才可以做加法)\n",
    "        #attn_output.shape:(batch_size,seq_len,d_model)\n",
    "        #out1.shape:(batch_size,seq_len,d_model)\n",
    "        attn_output,_=self.mha(x,x,x,encoder_padding_mask)\n",
    "        attn_output=self.dropout1(attn_output,training=training)\n",
    "        out1=self.layer_norml(x+attn_output)\n",
    "        \n",
    "        #ffn_output.shape:(batch_size,seq_len,d_model)\n",
    "        #out2.shape:(batch_size,seq_len,d_model)\n",
    "        ffn_output=self.ffn(out1)\n",
    "        ffn_output=self.dropout2(attn_output,training=training)\n",
    "        out2=self.layer_norml(out1+ffn_output)\n",
    "        \n",
    "        return out2\n",
    "        \n",
    "sample_encoder_layer=EncoderLayer(512,8,2048)\n",
    "sample_input=tf.random.uniform((64,50,512))\n",
    "sample_output=sample_encoder_layer(sample_input,False,None)\n",
    "print(sample_output.shape) #经过encoder后x的shape没有变\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 37, 512)\n"
     ]
    }
   ],
   "source": [
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self,num_layers,input_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "        super(EncoderModel,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_layers=num_layers\n",
    "        self.max_length=max_length\n",
    "        \n",
    "        self.embedding=keras.layers.Embedding(input_vocab_size,self.d_model)\n",
    "        #position_embedding.shape:(1,max_length,d_mdoel)\n",
    "        self.position_embedding=get_positional_embedding(max_length,self.d_model)\n",
    "        self.dropout=keras.layers.Dropout(rate)\n",
    "        self.encoder_layers=[EncoderLayer(d_model,num_heads,dff,rate)\n",
    "                             for _ in range(self.num_layers)]\n",
    "        \n",
    "    def call(self,x,training,encoder_padding_mask):\n",
    "        #x.shape:(batch_size,input_seq_len)\n",
    "        input_seq_len=tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal( input_seq_len,self.max_length,\" input_seq_len<=self.max_length\")\n",
    "        \n",
    "        #x.shape:(batch_size,input_seq_len,d_model)\n",
    "        x=self.embedding(x)\n",
    "        x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))#缩放，使x的作用更大些\n",
    "        x+=self.position_embedding[:,:input_seq_len,:]\n",
    "        x=self.dropout(x,training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x=self.encoder_layers[i](x,training,encoder_padding_mask)\n",
    "            \n",
    "        return x   \n",
    "        #x.shape:(batch_size,input_seq_len,d_modl)\n",
    "        \n",
    "sample_encoder_model=EncoderModel(2,8500,400,512,8,2048)\n",
    "\n",
    "sample_encoder_model_input=tf.random.uniform((64,37))\n",
    "sample_encoder_model_output=sample_encoder_model(sample_encoder_model_input,False,encoder_padding_mask=None)   \n",
    "print(sample_encoder_model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型把层当函数使用调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 512) (64, 8, 60, 60) (64, 8, 60, 50)\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x->self.attention->add&normalize&dropout->out1\n",
    "    out1,encoding_outputs->attention->add&normalize&dropout->out2\n",
    "    out2->ffn->add&normalize&dropout->out3\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,num_heads,dff,rate=0.1):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.mha1=MultiHeadAttention(d_model,num_heads)\n",
    "        self.mha2=MultiHeadAttention(d_model,num_heads)\n",
    "        \n",
    "        self.ffn=feed_forward_network(d_model,dff)\n",
    "        \n",
    "        self.layer_normal=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norma2=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.layer_norma3=keras.layers.LayerNormalization(\n",
    "                            epsilon=1e-6)\n",
    "        self.dropout1=keras.layers.Dropout(rate)\n",
    "        self.dropout2=keras.layers.Dropout(rate)\n",
    "        self.dropout3=keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self,x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask):\n",
    "        #decoder_mask:由look_ahead_mask和decoder_padding_mask合并而来\n",
    "        \n",
    "        #x.shape:(batch_size,target_seq_len,d_model)\n",
    "        #encoding_outputs.shape(batch_size,input_)\n",
    "        \n",
    "        #attn1,out1.shape:(batch_size,target_seq_len,d_model)\n",
    "        attn1,attn_weights1=self.mha1(x,x,x,decoder_mask)\n",
    "        attn1=self.dropout1(attn1,training=training)\n",
    "        out1=self.layer_normal(attn1+x)\n",
    "        \n",
    "        attn2,attn_weights2=self.mha2(\n",
    "        out1,encoding_outputs,encoding_outputs,encoder_decoder_padding_mask)\n",
    "        \n",
    "        attn2=self.dropout2(attn2,training=training)\n",
    "        out2=self.layer_norma2(attn2+out1)\n",
    "        \n",
    "        ffn_output=self.ffn(out2)\n",
    "        ffn_output=self.dropout3(ffn_output,training=training)\n",
    "        out3=self.layer_norma3(ffn_output+out2)\n",
    "        \n",
    "        return out3,attn_weights1,attn_weights2\n",
    "    \n",
    "sample_decoder_layer=DecoderLayer(512,8,2048)\n",
    "sample_decoder_input=tf.random.uniform((64,60,512))\n",
    "sample_decode_output,sample_decoder_attn_weights1,sample_decoder_attn_weights2=sample_decoder_layer(sample_decoder_input,sample_output,False,None,None)\n",
    "\n",
    "\n",
    "print(sample_decode_output.shape,sample_decoder_attn_weights1.shape,sample_decoder_attn_weights2.shape)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 35, 512)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n"
     ]
    }
   ],
   "source": [
    "class DecoderModel(keras.layers.Layer):\n",
    "        def __init__(self,num_layers,target_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "            super(DecoderModel,self).__init__()\n",
    "            self.d_model=d_model\n",
    "            self.num_layers=num_layers\n",
    "            self.max_length=max_length\n",
    "\n",
    "            self.embedding=keras.layers.Embedding(target_vocab_size,self.d_model)\n",
    "            #position_embedding.shape:(1,max_length,d_mdoel)\n",
    "            self.position_embedding=get_positional_embedding(max_length,self.d_model)\n",
    "            self.dropout=keras.layers.Dropout(rate)\n",
    "            self.decoder_layers=[DecoderLayer(d_model,num_heads,dff,rate)\n",
    "                                 for _ in range(self.num_layers)]\n",
    "        def call(self,x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask):\n",
    "            #x.shape:(batch_size,out_seq_len)\n",
    "            output_seq_len=tf.shape(x)[1]\n",
    "            tf.debugging.assert_less_equal(output_seq_len,self.max_length,\"output_seq_len<=self.max_length\") \n",
    "            \n",
    "            attention_weights={}\n",
    "            \n",
    "            x=self.embedding(x)\n",
    "            x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
    "            x+=self.position_embedding[:,:output_seq_len,:]\n",
    "            x=self.dropout(x,training=training)\n",
    "            \n",
    "            for i in range(self.num_layers):\n",
    "                x,attn1,attn2=self.decoder_layers[i](x,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask)\n",
    "                attention_weights['decoder{}_att1'.format(i+1)]=attn1\n",
    "                attention_weights['decoder{}_att2'.format(i+1)]=attn2\n",
    "                                  \n",
    "            #x.shape:(batch_size,out_seq_len,d_,model)\n",
    "            return x,attention_weights\n",
    "                                  \n",
    "sample_decoder_model  =DecoderModel(2,8000,1000,512,8,2048)                                \n",
    "                                  \n",
    "sample_decoder_model_input=tf.random.uniform((64,35))\n",
    "sample_decoder_model_output,sample_decoder_model_att=sample_decoder_model(\n",
    "                    sample_decoder_model_input,\n",
    "                   sample_encoder_model_output,\n",
    "training=False,decoder_mask=None,encoder_decoder_padding_mask=None)\n",
    "                                  \n",
    "print(sample_decoder_model_output.shape)\n",
    "for key in sample_decoder_model_att:\n",
    "    print(sample_decoder_model_att[key].shape)#DecoderModel3层，所以attn有4个，35,35是self-attn，35,37是encoder-decoder atten\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 31, 8000)\n",
      "decoder1_att1 (64, 8, 31, 31)\n",
      "decoder1_att2 (64, 8, 31, 26)\n",
      "decoder2_att1 (64, 8, 31, 31)\n",
      "decoder2_att2 (64, 8, 31, 26)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(self,num_layers,input_vocab_size,target_vocab_size,max_length,d_model,num_heads,dff,rate=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.encoder_model=EncoderModel(num_layers,input_vocab_size,max_length\n",
    "                                       ,d_model,num_heads,dff,rate)\n",
    "        \n",
    "        self.decoder_model=DecoderModel(num_layers,input_vocab_size,max_length\n",
    "                                       ,d_model,num_heads,dff,rate)\n",
    "        self.final_layer=keras.layers.Dense(target_vocab_size)\n",
    "    def call(self,inp,tar,training,encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask):\n",
    "        \n",
    "        #encoding_outputs.shape:(batch_size,input-seq_len,d_model)\n",
    "        encoding_outputs=self.encoder_model(inp,training,encoder_padding_mask)\n",
    "        #decoding_outputs.shape:(batch_size,output_seq_len,target_vocab_\n",
    "        decoding_outputs,attention_weights=self.decoder_model(\n",
    "                    tar,encoding_outputs,training,decoder_mask,encoder_decoder_padding_mask)\n",
    "        \n",
    "        predictions=self.final_layer(decoding_outputs)\n",
    "        return predictions,attention_weights\n",
    "    \n",
    "sample_transformer=Transformer(2,8500,8000,1000,512,8,2048,rate=0.1)\n",
    "\n",
    "temp_input=tf.random.uniform((64,26))\n",
    "temp_target =tf.random.uniform((64,31))\n",
    "predictions,attention_weights=sample_transformer(temp_input,temp_target,\n",
    "                   training=False,\n",
    "                   encoder_padding_mask=None,\n",
    "                   decoder_mask=None,encoder_decoder_padding_mask=None)\n",
    "\n",
    "print(predictions.shape)\n",
    "for key in attention_weights:\n",
    "    print(key,attention_weights[key].shape)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.initializes model\n",
    "#2.define loss ,optimizer, learning_rate_schedule\n",
    "#3.train_step\n",
    "#4.train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pk_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1ba7c735477a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0minput_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpk_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;31m#因为要加start和end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0men_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pk_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "num_layers=4\n",
    "d_model=128\n",
    "dff=512\n",
    "num_heads=8\n",
    "input_vocab_size=pk_tokenizer.vocab_size+2#因为要加start和end\n",
    "target_vocab_size=en_tokenizer.vocab_size+2\n",
    "dropout_rate=0.1\n",
    "transformer=Transformer(num_layers,\n",
    "                       input_vocab_size,\n",
    "                       target_vocab_size,\n",
    "                       max_length,\n",
    "                       d_model,num_heads,\n",
    "                       dff,\n",
    "                       dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrate=(d_model**-0.5)*min(step_num**(-0.5),\n",
    "#                  step_num*warm_up_steps**(-1.5) )\n",
    "\n",
    "class CustomizedSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,d_model,warmup_steps=4000):\n",
    "        super(CustomizedSchedule,self).__init__()\n",
    "        self.d_model=tf.cast(d_model,tf.float32)\n",
    "        self.warmup_steps=warmup_steps\n",
    "    \n",
    "    def __call__(self,step):\n",
    "        arg1=tf.math.rsqrt(step)\n",
    "        arg2=step*(self.warmup_steps**(-1.5))\n",
    "        \n",
    "        arg3=tf.math.rsqrt(self.d_model)\n",
    "        \n",
    "        return arg3*tf.math.minimum(arg1,arg2)\n",
    "learning_rate=CustomizedSchedule(d_model)\n",
    "optimizer=keras.optimizers.Adam(learning_rate,beta_1=0.9,beta_2=0.98,epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train step')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule=CustomizedSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000,dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Train step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object=keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True,reduction='none')#因为要对mask进行处理，不能用本身自带的reduction，所以要为none\n",
    "def loss_function(real,pred):\n",
    "    mask=tf.math.logical_not(tf.math.equal(real,0))#有padding的地方mask值都为零\n",
    "    loss_=loss_object(real,pred)\n",
    "    mask=tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_*=mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp,tar):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "        -encoder_padding_mask 计算attention时候的(self attention of EncoderLayer)\n",
    "    Decoder:\n",
    "        -look_ahead_mask 前面单词不能看到后面的(self attention of DecoderLayer)\n",
    "        -encoder_decoder_padding_mask  encoder_decoder之间的mask(encoder-decoder attention)\n",
    "        -decoder_padding_mask decoder上的attention (self attention of DecoderLayer)\n",
    "    \"\"\"\n",
    "    encoder_padding_mask=create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask=create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask=create_look_ahead_mask(tf.shape(tar)[1])#被mask是1，不被mask是零\n",
    "    decoder_padding_mask=create_padding_mask(tar)\n",
    "    decoder_mask=tf.maximum(decoder_padding_mask,look_ahead_mask)\n",
    "    print(encoder_padding_mask.shape)\n",
    "    print(encoder_decoder_padding_mask.shape)\n",
    "    print(look_ahead_mask.shape)#正方形矩阵，上三角是1，下三角是0\n",
    "    print(decoder_padding_mask.shape)\n",
    "    print(decoder_mask.shape)#tensor的自动补全机制，补上2个维度，把下一个维度复制39份\n",
    "    return encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inp,temp_tar=iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 16)\n",
      "(64, 1, 1, 16)\n",
      "(10, 10)\n",
      "(64, 1, 1, 10)\n",
      "(64, 1, 10, 10)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in converted code:\n\n    <ipython-input-41-f2d4432f3ba5>:13 train_step  *\n        predictions,_=transformer(inp,tar_inp,True,encoder_padding_mask,\n\n    NameError: name 'transformer' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f2d4432f3ba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 497\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: in converted code:\n\n    <ipython-input-41-f2d4432f3ba5>:13 train_step  *\n        predictions,_=transformer(inp,tar_inp,True,encoder_padding_mask,\n\n    NameError: name 'transformer' is not defined\n"
     ]
    }
   ],
   "source": [
    "train_loss=keras.metrics.Mean(name='train_loss')#定义累计的平均loss\n",
    "train_accuracy=keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp,tar):\n",
    "    tar_inp=tar[:,:-1]\n",
    "    tar_rel=tar[:,1:]\n",
    "    \n",
    "    encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask=\\\n",
    "                                create_masks(inp,tar_inp)\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions,_=transformer(inp,tar_inp,True,encoder_padding_mask,\n",
    "                                 decoder_mask,\n",
    "                                 encoder_decoder_padding_mask)\n",
    "        \n",
    "        loss=loss_function(tar_real,predictions)\n",
    "        \n",
    "    gradients=tape.gradient(loss,transformer.trainable_variables)#计算梯度\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))#将梯度绑定到变量上\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real,preictions)#计算累计 的accuracy\n",
    "    \n",
    "    \n",
    "epochs=20\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss.reset_states()#从零开始累计\n",
    "    train_accuracy.reset_states()\n",
    "    for (batch,(inp,tar)) in enumerate(train_dataset):\n",
    "        train_step(inp,tar)\n",
    "        \n",
    "        if batch%100==0:\n",
    "            print('Epoch{} Batch{} Loss{:.4f} Accuracy{:.4f}'.format(epoch+1,batch,train_losss.result(),\n",
    "                                                                    train_accuracy.result())\n",
    "                         )\n",
    "            \n",
    "    \n",
    "    \n",
    "    print('Epoch{}Loss{:.4f} Accuracy {:.4f}'.format(\n",
    "                    epoch+1,train_loss.result(),train_accuracy.result()))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eg:A B C D->E F G H.\n",
    "train: A B C D,E F G->F G H\n",
    "Eval:A B C D ->E\n",
    "    A B C D E->F\n",
    "    A B C D E F ->G\n",
    "    A B C D E F G->H    transform训练时并行处理，预测时候不是\n",
    "\"\"\"\n",
    "def evaluate(inp_sentence):\n",
    "    input_id_sentence=[pt_tokenizer.vacab_size]+pt_tokenizer.encode(inp_sentence)+[pt_tokenizer.vacab_size+1]#把文本的句子转化成id的句子\n",
    "    #encoder_input.shape:(1,input_sentence_length)\n",
    "    encoder_input=tf.expand_dims(input_id_sentence,0)\n",
    "    #decoder_input.shape:(1,1)\n",
    "    decoder_input=tf.expand_dims([en_tokenizer.vocab_size],0)\n",
    "    #transform 与seqtoseq不一样：transform中decoder_input为多少，就会给出多少个预测值（多步预测）。seqtoseq天生是单步预测，每次只得到一个值。\n",
    "    for i in range(max_length):\n",
    "        encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask\\\n",
    "        =create_masks(encoder_input,decoder_input)\n",
    "        #predictions.shape:(batch_size,output_target_len,target_vocab_size)\n",
    "        predictions,attention_weights=transformer(encoder_input,decoder_input,False,\n",
    "                                                  encoder_padding_mask,decoder_mask,encoder_decoder_padding_mask)\n",
    "        \n",
    "        #predictions.shape:(batch_size,target_vocab_size),中间维度只去一个值，维度消失\n",
    "        predictions=predictions[:,-1,:]#只需要最后一个预测值，就是decoder_nput全部输入进去得到的值\n",
    "        predicted_id=tf.cast(tf.argmax(predictions,axis=-1),tf.int32)\n",
    "        \n",
    "        if tf.equal(predicted_id,en_tokenizer.vocab_size+1):\n",
    "            return tf.squeeze(decoder_input,axis=0),attention_weights\n",
    "        \n",
    "        decoder_input=tf.concat([decoder_input,predicted_id],axis=-1)\n",
    "        \n",
    "    return tf.squeeze(decoder_input,axis=0),attention_weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units=256\n",
    "units=1024\n",
    "input_vocab_size=len(input_tokenizer.word_index)+1#??\n",
    "output_vocab_size=len(output_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [3,1024] vs. [64,1024] [Op:AddV2] name: encoder/gru/add/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-627ae9dff779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0msample_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0msample_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#输出的1024是状态是size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-627ae9dff779>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;31m#每一步的输出，以及最后一步输出的隐含状态\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'constants'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m   def call(self,\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[1;32m--> 422\u001b[1;33m           inputs, initial_state, training, mask, row_lengths)\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[1;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcudnn_gru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcudnn_gru_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandard_gru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnormal_gru_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m       last_output, outputs, new_h, runtime = gru_with_backend_selection(\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_gru\u001b[1;34m(inputs, init_h, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths)\u001b[0m\n\u001b[0;32m    565\u001b[0m       \u001b[0mgo_backwards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m       if sequence_lengths is not None else timesteps)\n\u001b[0m\u001b[0;32m    568\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_RUNTIME_CPU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[0;32m   4025\u001b[0m     \u001b[1;31m# the value is discarded.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4026\u001b[0m     output_time_zero, _ = step_function(\n\u001b[1;32m-> 4027\u001b[1;33m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[0;32m   4028\u001b[0m     output_ta = tuple(\n\u001b[0;32m   4029\u001b[0m         tensor_array_ops.TensorArray(\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[0;32m    548\u001b[0m     recurrent_z, recurrent_r, recurrent_h = array_ops.split(matrix_inner, 3,\n\u001b[0;32m    549\u001b[0m                                                             axis=1)\n\u001b[1;32m--> 550\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_z\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecurrent_z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_r\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecurrent_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[0mhh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_h\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrecurrent_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tensor\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,1024] vs. [64,1024] [Op:AddV2] name: encoder/gru/add/"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_units,encoding_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.encoding_units=encoding_units\n",
    "        self.embedding=keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru=keras.layers.GRU(self.encoding_units,return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform')    \n",
    "    def call(self,x,hidden):\n",
    "        x=self.embedding(x)\n",
    "        output,state=self.gru(x,initial_state=hidden)\n",
    "        return output,state #每一步的输出，以及最后一步输出的隐含状态      \n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units))\n",
    "    \n",
    "encoder=Encoder(input_vocab_size,embedding_units,units,batch_size)\n",
    "sample_hidden=encoder.initialize_hidden_state()\n",
    "sample_output,sample_hidden=encoder(x,sample_hidden)\n",
    "print(sample_output.shape)#输出的1024是状态是size\n",
    "print(sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.w1=keras.layers.Dense(units)\n",
    "        self.w2=keras.layers.Dense(units)\n",
    "        self.v=keras.layers.Dense(1)\n",
    "        \n",
    "        \n",
    "    def call(self,decoder_hidden,encoder_outputs):\n",
    "        #decoder_hidden.shape:(batch_size,units)\n",
    "        #encoder_outputs.shape:(batch_size,length,units)\n",
    "        decoder_hidden_with_time_axis=tf.expand_dims(decoder_hidden,1)\n",
    "        \n",
    "        #before v:(batch_size,length,units)\n",
    "        #after v:(batch_size,units)\n",
    "        score=self.v(\n",
    "            tf.nn.tanh(self.w1(encoder_outputs)+self.w2(decoder_hidden_with_time_axis)))\n",
    "        #shape:(batch_size,length,1)\n",
    "        attention_weights=tf.nn.softmax(score,axis=1)\n",
    "        #context_vector.shape:(batch_size,length,units)在最后一个维度做扩展了\n",
    "        context_vector=attention_weights*encoder_outputs\n",
    "        #context_vector.shape(batch_size,units)在length上求和，length维度就没有了？？\n",
    "        context_vector=tf.reduce_sum(context_vector,axis=1)\n",
    "        \n",
    "        return context_vector,attention_weights\n",
    "    \n",
    "attention_model=BahdanauAttention(units=10)\n",
    "attention_results,attention_weights=attention_model(sample_hidden,sample_output)\n",
    "\n",
    "print(attention_results.shape,attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_units,decoding_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.decoding_units=decoding_units\n",
    "        self.embedding=keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru=keras.layers.GRU(self.decoding_units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc=keras.layers.Dense(vocab_size)\n",
    "        self.attention=BahdanauAttention(self.decoding_units)\n",
    "        \n",
    "        \n",
    "    def call(self,x,hidden,encoding_outputs):\n",
    "        #context_vector.shape:(batch_size,units)\n",
    "        context_vector,attention_weights=self.attention(hidden,encoding_outputs)\n",
    "        #befor embedding:x.shape:(batch,1)decoding是单步的decoding\n",
    "        #after embedding:x.shape:(batch_size,1,embedding_units)\n",
    "        x=self.embedding(x)\n",
    "        \n",
    "        combined_x=tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)#在最后一个维度上拼接\n",
    "        \n",
    "        #output.shape:[batch_size,1,decoding_units]\n",
    "        #state.shape:[batch_size,decoding_units]\n",
    "        \n",
    "        output,state=self.gru(combined_x)\n",
    "        #output.shape:[batch_size,decoding_units]\n",
    "        output=tf.reshape(output,(-1,output.shape[2]))\n",
    "        \n",
    "        #output.shape:[batch_size,vocab_size]\n",
    "        output=self.fc(output)\n",
    "        \n",
    "        return output,state,attention_weights\n",
    "        \n",
    "        \n",
    "decoder=Decoder(output_vocab_size,embedding_units,units,batch_size)\n",
    "\n",
    "outputs=decode(tf.random.uniform((batch_size,1)),sample_hidden,sample_output)\n",
    "decode_output,decoder_hidden,decoder_aw=outputs\n",
    "\n",
    "print(decode_output.shape,decoder_hidden.shape,decoder_aw.shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer=keras.optimizers.Adam()\n",
    "\n",
    "loss_object=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')# reduction表示损失函数该如何聚                                                     \n",
    "        \n",
    "def loss_function(real,pred):\n",
    "    mask= tf.math.logical_not(tf.math.equal(real,0))# tf.math.equal(real,0) 是0的返回True，目标是让mask中不是零的变成1，其余为0          #mask输出的时候有很多padding，padding不应该计算在损失函数中去，把padding对应的损失函数对应的改为零\n",
    "    loss_=loss_object(real,pred)\n",
    "    mask=tf.cast(mask,dtype=loss_.dtype)#把mask布尔数据类型，转化成loss相同的\n",
    "    \n",
    "    loss_*=mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)# reduction='None'先不具合，乘完以后再聚合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp,targ,encoding_hidden):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs,encoding_hidden=encoder(inp,encoding_hidden)\n",
    "        decoding_hidden=encoding_hidden\n",
    "        for t in range(0,targ.shape[1]-1):\n",
    "            decoding_input=tf.expand_dims(targ[:,t],1)\n",
    "            predictions,decoding_hidden,_=decoder(decoding_input,decoding_hidden,\n",
    "                                                 encoding_outputs)\n",
    "            loss+=loss_function(targ[:,t+1],predictions)\n",
    "            \n",
    "    batch_loss=loss/int(targ.shape[0])\n",
    "    variables=encoder.trainable_variables+decoder.trainable_variables\n",
    "    gradients=tape.gradient(loss,variables)\n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "    \n",
    "    return batch_loss\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10#循环遍历10次数据集\n",
    "steps_per_epoch=len(input_tensor)//batch_size#遍历数据集一次\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    encoding_hidden=encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss=0\n",
    "    \n",
    "    for (batch,(inp,targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss=train_step(inp,targ,encoding_hidden)\n",
    "        total_loss+=batch_loss\n",
    "        \n",
    "        if batch%100==0:\n",
    "            print('Epoch{} Batch {} Loss {:.4f}'.format(epoch+1,batch,batch_loss.numpy()))\n",
    "        \n",
    "    \n",
    "    print('Epoch{} Batch {} Loss {:.4f}'.format(epoch+1,batch,batch_loss.numpy()))\n",
    "    print('Time take for 1 epoch {} sec'.format(time.time()-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<END>'] = 3\n",
    "\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for key, value in word_index.items()])\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join(\n",
    "        [reverse_word_index.get(word_id, \"<UNK>\") for word_id in text_ids])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix=np.zeros((max_length_output,max_length_input))\n",
    "    input_sentence=preprocess_sentence(input_sentence)\n",
    "    inputs=[input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs=keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs],maxlen=max_length_input,padding='post'\n",
    "    )\n",
    "    inputs=tf.convert_to_tensor(inputs)#保存成tensor\n",
    "    \n",
    "    results=''\n",
    "#     encoding_hidden=encoder.initialize_hidden_state()\n",
    "    encoding_hidden=tf.zeros((1,units))    \n",
    "    encoding_outputs,encoding_hidden=encoder(inputs,encoding_hidden)\n",
    "    decoding_hidden=encoding_hidden\n",
    "    \n",
    "    #eg:<start>->A\n",
    "    #A->B->C->D\n",
    "    #decoding_input.shape:(1,1)\n",
    "    decoding_input=tf.expand_dims(output_tokenizer.word_index['<start>'],0)\n",
    "    for t in range(max_length_output):\n",
    "        pedictions,decoding_hidden,attention_weights=decoder(decoding_input,decoder_hidden,encoding_outputs)\n",
    "        \n",
    "        #attention_weights.shape:(batch_size,input_length,1)(1,16,1)\n",
    "        attention_weights=tf.reshape(attention_weights,(-1,))#得到长度为16的向量？\n",
    "        attention_matrix[t]=attention_matrix.numpy()\n",
    "        \n",
    "        #predictions.shape:(batch_size,vocab_size)(1,4935)\n",
    "        predicted_id=tf.argmax(predictions[0].numpy())\n",
    "        \n",
    "        results+=output_tokenizer.index_word[predicted_id]+' '\n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id]=='<end>':\n",
    "            return results,input_sentence,attention_matrix\n",
    "            \n",
    "        \n",
    "        decoding_input=tf.expand_dims([predicted_id],0)\n",
    "        \n",
    "    return results,input_sentence,attention_matrix\n",
    " \n",
    "\n",
    "def plot_attention(attention_matrix,input_sentence,predicted_sentence):\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    ax=fig,add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix,cmap='viridis')\n",
    "    font_dict={'fontsize':14}\n",
    "    \n",
    "    ax.set_xticklabels(['']+input_sentence,font_dict=font_dict,rotation=90)\n",
    "    ax.set_yticklabels(['']+predicted_sentence,fontdict=font_dict)\n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results,input_sentence,attention_matrix=evaluate(input_sentence)\n",
    "    \n",
    "    print(input_sentence)\n",
    "    print(results)\n",
    "    \n",
    "    attention_matrix=attention_matrix[:len(results.split(\" \")),:len(input_sentence.split(' '))]\n",
    "    \n",
    "    plot_attention(attention_matrix,input_sentence.split(' '),results.split(\" \"))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Te amo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data, # list of list\n",
    "    value = word_index['<PAD>'],\n",
    "    padding = 'post', # post, pre\n",
    "    maxlen = max_length)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data, # list of list\n",
    "    value = word_index['<PAD>'],\n",
    "    padding = 'post', # post, pre\n",
    "    maxlen = max_length)\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "single_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.LSTM(units = 64, return_sequences = False),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "single_rnn_model.summary()\n",
    "single_rnn_model.compile(optimizer = 'adam',\n",
    "                         loss = 'binary_crossentropy',\n",
    "                         metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_single_rnn = single_rnn_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 30,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history_single_rnn, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history_single_rnn, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_rnn_model.evaluate(\n",
    "    test_data, test_labels,\n",
    "    batch_size = batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 64, return_sequences = True)),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 64, return_sequences = False)),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 30,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 512\n",
    "\n",
    "bi_rnn_model = keras.models.Sequential([\n",
    "    # 1. define matrix: [vocab_size, embedding_dim]\n",
    "    # 2. [1,2,3,4..], max_length * embedding_dim\n",
    "    # 3. batch_size * max_length * embedding_dim\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           input_length = max_length),\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "            units = 32, return_sequences = False)),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "bi_rnn_model.summary()\n",
    "bi_rnn_model.compile(optimizer = 'adam',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bi_rnn_model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs = 5,\n",
    "    batch_size = batch_size,\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn_model.evaluate(test_data, test_labels, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
